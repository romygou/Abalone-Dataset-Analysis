---
title: "Abalone Dataset Analysis Project"
author: "Romy Gou"
date: "`r Sys.Date()`"
output: rmarkdown::github_document
---

```{r, echo = FALSE}
suppressMessages(library(car))
suppressMessages(library(leaps))
abalone <- read.table("abalone.data", sep = ",")
colnames(abalone) <- c("Sex", "Length", "Diameter", "Height", "Whole", "Shucked", "Viscera", "Shell", "Rings")
```

# Exposition

Similar to trees, abalone shells form distinct growth rings as it ages. These rings can be counted to estimate the age of the abalone, with each ring representing a period of growth. Through a method that involves cutting the shell, staining it, and counting the number of rings through a microscope, scientists can determine the age of an abalone.

But as this method is often tedious and time-consuming, are there other physical attributes through which we can more efficiently estimate an abalone's age? If so, which factors contribute most towards this estimation, and which could we do without?

# The Data Set

For this project, we will be using the Abalone data set from UC Irvine's Machine Learning Repository, donated in 1995 by Warwick Nash, Tracy Sellers, Simon Talbot, Andrew Cawthorn, and Wes Ford. This data set contains nine columns of data:

- Sex: The sex of the abalone (M = male, F = female, I = infant)

- Length: The longest shell measurement, in millimeters
 
- Diameter: The diameter of the abalone taken perpendicular to length, in millimeters

- Height: The height of the abalone with meat in the shell, in millimeters

- Whole: The weight of the whole abalone, in grams

- Shucked: The weight of the meat of the abalone, in grams

- Viscera: The weight of the gut of the abalone after bleeding, in grams

- Shell: The weight of the shell of the abalone after being dried, in grams

- Rings: The number of rings on the shell, adding 1.5 gives the estimated age in years

Being the value that is most often used to predict age, the number of rings will be the value that this project attempts to predict, i.e. the response variable.

# Cleaning the Data

There isn't much cleaning to perform on this data set, given that the data had been cleaned before its donation. However, since Sex is a categorical variable, we need to make dummy variables for it. Sex has 3 levels, so we will need to create 3-1 dummy variables. We will choose the category "Infant" to be the reference group.

```{r}
# dummy variables
abalone$Male <- abalone$Sex
abalone$Female <- abalone$Sex

abalone$Male[abalone$Male == "M"] <- 1
abalone$Male[abalone$Male == "F" | abalone$Male == "I"] <- 0
abalone$Male <- as.numeric(abalone$Male)
abalone$Female[abalone$Female == "F"] <- 1
abalone$Female[abalone$Female == "M" | abalone$Female == "I"] <- 0
abalone$Female <- as.numeric(abalone$Female)

# remove Sex and rearrange columns
abalone <- abalone[,-1]
abalone <- abalone[,c(9,10,1:8)]
```

# Choosing Predictors

Not all predictors are necessary to create the best model for predicting the response; in some cases, including too many predictors can actually degrade the quality of our predictions, in a problem known as overfitting. To prevent this issue, we will first analyze the relationships between our predictors to see if variable selection is required before fitting a model.

```{r figs, fig.width = 6, fig.height = 4.5}
plot(abalone[,1:9])
```

Figure 1 shows that several of the predictor variables seem to be linearly correlated. Many of these linear relationships make sense intuitively, such as that of the whole weight and shucked weight.

```{r}
round(cor(abalone[,1:9]), 4)
```

Similarly, the correlation matrix shows that several of the predictors have a near-perfect degree of linear correlation between them. This suggests that some predictors may not be necessary in the final model, as their effect on the response variable may be accounted for via the other variables.

```{r}
m1 <- lm(Rings ~ Male + Female + Length + Diameter + Height + Whole + Shucked + Viscera +
           Shell, data = abalone)
summary(m1)
```

Additionally, the summary suggests while the overall test is highly statistically significant, not all of the coefficients are statistically significant.

```{r}
vif(m1)
```

Lastly, we will analyze the VIFs for the predictors. The VIF of a predictor measures how easily it can be predicted from a linear regression using the other predictors—a general guideline is that a VIF is large when it is greater than 5. For this data set, the VIFs show that several of the predictors have multicollinearity.\

\
Thus, we will need to choose a subset of the predictors using variable selection.

# Variable Selection

With nine predictors, there are many possible models for fitting to the response variable, rings—512 models, to be exact! To find the best model, we will employ a variable selection technique. We will use all possible subsets selection.

```{r}
X <- as.matrix(abalone[,1:9])
b <- regsubsets(as.matrix(X), abalone$Rings)
summary(b)
```

Through all possible subsets selection, we've found the optimal models for all subset sizes:

- $p = 1$: Shell

- $p = 2$: Shucked, Shell

- $p = 3$: Diameter, Shucked, Shell

- $p = 4$: Diameter, Whole, Shucked, Shell

- $p = 5$: Diameter, Height, Whole, Shucked, Viscera

- $p = 6$: Diameter, Height, Whole, Shucked, Viscera, Shell

- $p = 7$: Male, Female, Diameter, Whole, Shucked, Viscera, Shell

- $p = 8$: Male, Female, Diameter, Height, Whole, Shucked, Viscera, Shell

- $p = 9$: all nine predictors\

Next, we will select the best model from these eight models. To do so, we will compare their goodness of fit criteria.

```{r}
# list of subsets for looping
set1 <- abalone[,9]
set2 <- abalone[,c(7,9)]
set3 <- abalone[,c(4,7,9)]
set4 <- abalone[,c(4,6,7,9)]
set5 <- abalone[,c(4,5,6,7,8)]
set6 <- abalone[,c(4,5,6,7,8,9)]
set7 <- abalone[,c(1,2,4,6,7,8,9)]
set8 <- abalone[,c(1,2,4,5,6,7,8,9)]
set9 <- abalone[,c(1:9)]
model_list <- list(set1, set2, set3, set4, set5, set6, set7, set8, set9)

# empty matrix to store data
model_mat <- matrix(rep(0, 36), nrow = 9, ncol = 4)
rownames(model_mat) <- c("1", "2", "3", "4", "5", "6", "7", "8", "9")
colnames(model_mat) <- c("Adjusted R^2", "AIC", "Corrected AIC", "BIC")

for(set in 1:9) {
  om <- lm(abalone$Rings ~ as.matrix(model_list[[set]]))
  p <- set
  n <- nrow(abalone)
  
  # adjusted R^2
  R_adj <- summary(om)$adj.r.squared
  model_mat[set, 1] <- R_adj
  
  # AIC
  AIC <- extractAIC(om)[2]
  model_mat[set, 2] <- AIC
  
  # corrected AIC
  AIC_corrected <- AIC + ((2*(p+2)*(p+3)) / (n-p-1))
  model_mat[set, 3] <- AIC_corrected
  
  # BIC
  BIC <- extractAIC(om, k = log(n))[2]
  model_mat[set, 4] <- BIC
}

model_mat
```

The table shows three different metrics. For $R_{adj}^2$, a higher value is desirable. For $AIC$ and $BIC$, a lower value is desirable. 

Thus, $R_{adj}^2$, $AIC$, corrected $AIC$, and $BIC$ all suggest that the subset with $p = 8$ makes the best model.

\

Therefore, the best model is:

$Y = \beta_0 + \beta_1 Male + \beta_2 Female + \beta_3 Diameter + \beta_4 Height + \beta_5 Whole + \beta_6 Shucked + \beta_7 Viscera + \beta_8 Shell + e$

```{r}
om8 <- lm(Rings ~ Male + Female + Diameter + Height + Whole + Shucked + Viscera + Shell,
          data = abalone)
```

# Model Diagnostics

Now that we have an optimal model, we can use various diagnostic tools to evaluate the model and then investigate if observations with large influences are present. Note that in the real world, not all data will meet all the regression assumptions.

### Multicollinearity

```{r}
vif(om8)
```

Several of the predictors still have high VIF values. This may suggest that the parts of the predictors that are independent of one another are significant in predicting the response, and their predictive value may be more important than avoiding multicollinearity.

### Linearity

The first regression assumption is linearity. We will check if the response variable Rings is linearly associated with the chosen predictors.

```{r}
plot(abalone[c(1:2, 4:10)])
```

The scatterplot matrix shows that the response appears to be linearly associated with the predictors, except the predictor Sex.

### Equal Variances

The second assumption is equal variances. We will check if the variance of the residuals is the same for all values of the predictor by plotting the standardized residual against each of the predictors.

```{r}
par(mfrow = c(2,4))
plot(abalone$Male, rstandard(om8), ylab = "Standardized Residuals")
abline(h = 0, col = "red")
plot(abalone$Female, rstandard(om8), ylab = "Standardized Residuals")
abline(h = 0, col = "red")
plot(abalone$Diameter, rstandard(om8), ylab = "Standardized Residuals")
abline(h = 0, col = "red")
plot(abalone$Height, rstandard(om8), ylab = "Standardized Residuals")
abline(h = 0, col = "red")
plot(abalone$Whole, rstandard(om8), ylab = "Standardized Residuals")
abline(h = 0, col = "red")
plot(abalone$Shucked, rstandard(om8), ylab = "Standardized Residuals")
abline(h = 0, col = "red")
plot(abalone$Viscera, rstandard(om8), ylab = "Standardized Residuals")
abline(h = 0, col = "red")
plot(abalone$Shell, rstandard(om8), ylab = "Standardized Residuals")
abline(h = 0, col = "red")
```

The plots show that all predictors except Height seem to have constant variability.

### Normality of Errors

The third assumption is normality of errors. We will check if the residuals are approximately normally distributed by examining a normal probability plot.

```{r}
plot(om8, which = 2)
```

Since the observations mostly fall near the line, this assumption is satisfied.

### Independence of Errors

The last assumption is independence of errors. We will check if there is a relationship between the residuals and the response variable by examining a scatterplot of residuals vs. fits.

```{r}
plot(om8, which = 1)
```

Since the correlation is approximately zero, this assumption is satisfied.

### Outlier Analysis

Lastly, we should check for highly-influential observations, which may have an impact on the regression estimations. We will use three methods to check for outliers: leverage, standardized residuals, and Cook's distance.

```{r}
influencePlot(om8)
```

We have identified four influential/unusual points and will remove them from our model.

```{r}
model_full <- lm(Rings ~ Male + Female + Length + Diameter + Height + Whole + Shucked +
                   Viscera + Shell, data = abalone[c(-481,-1418,-2052,-2628),])
model_reduced <- lm(Rings ~ Male + Female + Diameter + Height + Whole + Shucked + Viscera
                    + Shell, data = abalone[c(-481,-1418,-2052,-2628),])
```

# Inference

### T-Test

The t-test is the marginal impact of the predictors given the presence of all other predictors.

```{r}
summary(model_reduced)
```

As previously stated, the p-value for this model is extremely small, so all chosen predictor variables are significantly related to the response.

The $R^2_{adj}$ value represents the proportion of the variance in the response that can be explained by the predictor variables. In this case, 54.57% of the variance in the response can be explained, suggesting that the predictors don't exert a strong amount of influence on the response.

Note that since many of the predictors had high VIF values, the regression estimates become more unreliable.

### F-Test

The F-test is sequential, so it tests for the importance of each predictor without the presence of the other predictors.

```{r}
anova(model_reduced)
```

With ANOVA, the p-values for all predictors are also extremely small. This supports the conclusion that the predictors have a significant effect on the response.

### Partial F-Test

Lastly, we will conduct a partial F-test to determine the better model between the full and reduced model:

```{r}
anova(model_reduced, model_full)
```

Since the p-value is 0.689, we cannot reject the null hypothesis that the coefficient Length = 0. It appears that Length does not contribute significant information to Rings once the other predictors have been taken into consideration. The reduced model is indeed the better model.
